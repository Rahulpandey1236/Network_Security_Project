ś## in lec 1 -> create conda environment using > create conda -p venv python==3.10
## now create all the folder structure of projects 

## __init__ .py is used to create a folder as a package or a module 
## pip install -r requirements.txt



## Lec 2->  Upload on Github and create __init__.py in every flder under network security

## lec 3 => write code in setup.py after print requirements test this it will print requirements im termianl
## write setup => ## its just referring to setup.py build package from setup.py
## pip install -r requirements.txt


## LEC 4=> now logging and exception help us to debugging 
## now create a file under logging logger.py 
## create exception.py 
## runexceotion.py using cd networksecurity cd exception pyhton exception.py


## lec 5=> undrtand project structure and about ETL pipeline 

## lec 6 => setup mongodb atlas =>

## lec 7=>in pus_data.py
## ETL completeion and check mongodb atlas in browse collection the data will show in json format 
## runs push_data.py =>  make push_data.py

## LEC 8=> undersanding about data ingestiion config artifacts folder and other structure about project

## LEc9=> data ingestion.py and in entity create config_entity.py and write code in entity config file 
-< in constant folder ceate a foldeer trainig pipeline  => write code in __init__.py => inthis we have some constant value  in trainig_pipelnine fodler 
## write code in config_entity.py    ## till here dataingestionconfig and in trainig_pipeline __init__.py is done ..... 

ś
## LEC 10 =>    write code in components data_ingestion.pyś
## make artifact_entity.py under entity folder
## create main.py out of every folder  
## run main.py


## lec 11 => data validation = > now complete config entity under entity folder for validai=tion data also update var for validation datad
## in compnts make data_validation.py 
## complete data_validation.py
## write code in artifact_entity 
## create a folder data_schema in makes a file schema.yaml file
## in utils make a fodler make_uls make a file utils.py then write in this file  


## lec 12 => data_validation part 2 => complete all the data_vlaidaion.py and utils.py file 
till here data_valiation completed

## lec13=> data transformation => complete config_entity for data_transformation
complete artifact_entity for data_transformation
## update varable trainig_pipeline __init__.py
## make data_transformation.py and complete it 
## also complete utis .py 


## lec14 => data_transformation imp part2 => complete the data_transformation and run pyhton main,py

##lec 15 => make model_trainer.py under components and complete it like other pipelines 
# make a flder ml_utils under utils
## create two folder metric and model under ml_utils

